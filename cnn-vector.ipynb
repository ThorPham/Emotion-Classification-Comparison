{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "\n",
    "class Model_vec:\n",
    "    \n",
    "    def __init__(self, batch_size, dimension_size, learning_rate, vocabulary_size):\n",
    "        self.train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        self.train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, dimension_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, self.train_inputs)\n",
    "        self.nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, dimension_size], stddev = 1.0 / np.sqrt(dimension_size)))\n",
    "        self.nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        self.loss = tf.reduce_mean(tf.nn.nce_loss(weights = self.nce_weights, biases = self.nce_biases, labels = self.train_labels,\n",
    "                                                  inputs=embed, num_sampled = batch_size / 2, num_classes = vocabulary_size))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        self.norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        self.normalized_embeddings = embeddings / self.norm\n",
    "        \n",
    "class Model:\n",
    "    def __init__(self, sequence_length, dimension_input, dimension_output, \n",
    "                 learning_rate, filter_sizes, out_dimension):\n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, sequence_length, dimension_input, 1])\n",
    "        self.Y = tf.placeholder(tf.float32, shape=[None, dimension_output])\n",
    "        pooled_outputs = []\n",
    "        for i in filter_sizes:\n",
    "            w = tf.Variable(tf.truncated_normal([i, dimension_input, 1, out_dimension], stddev=0.1))\n",
    "            b = tf.Variable(tf.truncated_normal([out_dimension], stddev = 0.01))\n",
    "            conv = tf.nn.relu(tf.nn.conv2d(self.X, w, strides=[1, 1, 1, 1],padding=\"VALID\") + b)\n",
    "            pooled = tf.nn.max_pool(conv,ksize=[1, sequence_length - i + 1, 1, 1],strides=[1, 1, 1, 1],padding='VALID')\n",
    "            pooled_outputs.append(pooled)\n",
    "        h_pool = tf.concat(pooled_outputs, 3)\n",
    "        h_pool_flat = tf.nn.dropout(tf.reshape(h_pool, [-1, out_dimension * len(filter_sizes)]), 0.1)\n",
    "        w = tf.Variable(tf.truncated_normal([out_dimension * len(filter_sizes), dimension_output], stddev=0.1))\n",
    "        b = tf.Variable(tf.truncated_normal([dimension_output], stddev = 0.01))\n",
    "        self.logits = tf.matmul(h_pool_flat, w) + b\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
    "        l2 = sum(0.0005 * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "        self.cost += l2\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = re.sub('[^\\'\\\"A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    return ' '.join(string)\n",
    "\n",
    "def read_data():\n",
    "    list_folder = os.listdir('data/')\n",
    "    label = list_folder\n",
    "    label.sort()\n",
    "    outer_string, outer_label = [], []\n",
    "    for i in range(len(list_folder)):\n",
    "        list_file = os.listdir('data/' + list_folder[i])\n",
    "        strings = []\n",
    "        for x in range(len(list_file)):\n",
    "            with open('data/' + list_folder[i] + '/' + list_file[x], 'r') as fopen:\n",
    "                strings += fopen.read().split('\\n')\n",
    "        strings = list(filter(None, strings))\n",
    "        for k in range(len(strings)):\n",
    "            strings[k] = clearstring(strings[k])\n",
    "        labels = [i] * len(strings)\n",
    "        outer_string += strings\n",
    "        outer_label += labels\n",
    "    \n",
    "    dataset = np.array([outer_string, outer_label])\n",
    "    dataset = dataset.T\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    string = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        string += dataset[i][0].split()\n",
    "    \n",
    "    return string, dataset, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, vocabulary_size):\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) + 1\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        data.append(index)\n",
    "    dictionary['PAD'] = 0\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, dictionary, reverse_dictionary\n",
    "\n",
    "def generate_batch_skipgram(words, batch_size, num_skips, skip_window):\n",
    "    data_index = 0\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for i in range(span):\n",
    "        buffer.append(words[data_index])\n",
    "        data_index = (data_index + 1) % len(words)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(words[data_index])\n",
    "        data_index = (data_index + 1) % len(words)\n",
    "    data_index = (data_index + len(words) - span) % len(words)\n",
    "    return batch, labels\n",
    "\n",
    "def generatevector(dimension, batch_size, skip_size, skip_window, num_skips, iteration, words_real):\n",
    "    \n",
    "    print (\"Data size:\", len(words_real))\n",
    "    data, dictionary, reverse_dictionary = build_dataset(words_real, len(words_real))\n",
    "    sess = tf.InteractiveSession()\n",
    "    print (\"Creating Word2Vec model..\")\n",
    "    model = Model_vec(batch_size, dimension, 0.1, len(dictionary))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    last_time = time.time()\n",
    "    for step in range(iteration):\n",
    "        new_time = time.time()\n",
    "        batch_inputs, batch_labels = generate_batch_skipgram(data, batch_size, num_skips, skip_window)\n",
    "        feed_dict = {model.train_inputs: batch_inputs, model.train_labels: batch_labels}\n",
    "        _, loss = sess.run([model.optimizer, model.loss], feed_dict=feed_dict)\n",
    "        if ((step + 1) % 1000) == 0:\n",
    "            print (\"epoch:\", step + 1, \", loss:\", loss, \", speed:\", (time.time() - new_time) * 1000, \"s / 1000 epoch\")\n",
    "    tf.reset_default_graph()       \n",
    "    return dictionary, reverse_dictionary, model.normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string, data, label = read_data()\n",
    "maxlen = 50\n",
    "location = os.getcwd()\n",
    "dimension = 512\n",
    "skip_size = 8\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "iteration_train_vectors = 20000\n",
    "size_layer = 128\n",
    "learning_rate = 0.0001\n",
    "epoch = 100\n",
    "filter_sizes = [2,3,4,5]\n",
    "batch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 8007324\n",
      "Creating Word2Vec model..\n",
      "epoch: 1000 , loss: 80.2528 , speed: 75.94656944274902 s / 1000 epoch\n",
      "epoch: 2000 , loss: 41.7089 , speed: 76.14827156066895 s / 1000 epoch\n",
      "epoch: 3000 , loss: 52.9637 , speed: 76.32899284362793 s / 1000 epoch\n",
      "epoch: 4000 , loss: 28.7064 , speed: 76.06983184814453 s / 1000 epoch\n",
      "epoch: 5000 , loss: 36.4879 , speed: 76.02572441101074 s / 1000 epoch\n",
      "epoch: 6000 , loss: 91.817 , speed: 76.03287696838379 s / 1000 epoch\n",
      "epoch: 7000 , loss: 12.3311 , speed: 76.02500915527344 s / 1000 epoch\n",
      "epoch: 8000 , loss: 18.6721 , speed: 76.28488540649414 s / 1000 epoch\n",
      "epoch: 9000 , loss: 14.3546 , speed: 76.00998878479004 s / 1000 epoch\n",
      "epoch: 10000 , loss: 9.51918 , speed: 76.01237297058105 s / 1000 epoch\n",
      "epoch: 11000 , loss: 32.0798 , speed: 75.96468925476074 s / 1000 epoch\n",
      "epoch: 12000 , loss: 21.1706 , speed: 75.92940330505371 s / 1000 epoch\n",
      "epoch: 13000 , loss: 12.1358 , speed: 76.35140419006348 s / 1000 epoch\n",
      "epoch: 14000 , loss: 32.3225 , speed: 75.99568367004395 s / 1000 epoch\n",
      "epoch: 15000 , loss: 0.28076 , speed: 75.94799995422363 s / 1000 epoch\n",
      "epoch: 16000 , loss: 32.6561 , speed: 76.06220245361328 s / 1000 epoch\n",
      "epoch: 17000 , loss: 22.2233 , speed: 76.0347843170166 s / 1000 epoch\n",
      "epoch: 18000 , loss: 10.1961 , speed: 76.58648490905762 s / 1000 epoch\n",
      "epoch: 19000 , loss: 38.1199 , speed: 75.94537734985352 s / 1000 epoch\n",
      "epoch: 20000 , loss: 9.88957 , speed: 76.13229751586914 s / 1000 epoch\n"
     ]
    }
   ],
   "source": [
    "dictionary, reverse_dictionary, vectors = generatevector(dimension, 32, skip_size, skip_window, num_skips, iteration_train_vectors, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data[:, 0], data[:, 1], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , pass acc: 0 , current acc: 0.363817515332\n",
      "epoch: 1 , training loss: 2.67426815545 , training acc: 0.313653258889 , valid loss: 2.28055589168 , valid acc: 0.363817515332\n",
      "epoch: 1 , pass acc: 0.363817515332 , current acc: 0.4648739341\n",
      "epoch: 2 , training loss: 2.04597621702 , training acc: 0.408917203836 , valid loss: 1.82930580851 , valid acc: 0.4648739341\n",
      "epoch: 2 , pass acc: 0.4648739341 , current acc: 0.627719067797\n",
      "epoch: 3 , training loss: 1.61223239937 , training acc: 0.545242933622 , valid loss: 1.40271844684 , valid acc: 0.627719067797\n",
      "epoch: 3 , pass acc: 0.627719067797 , current acc: 0.736494580094\n",
      "epoch: 4 , training loss: 1.23305273374 , training acc: 0.68751947765 , valid loss: 1.09067239238 , valid acc: 0.736494580094\n",
      "epoch: 4 , pass acc: 0.736494580094 , current acc: 0.792737075833\n",
      "epoch: 5 , training loss: 0.987592778416 , training acc: 0.769259131162 , valid loss: 0.907978268517 , valid acc: 0.792737075833\n",
      "epoch: 5 , pass acc: 0.792737075833 , current acc: 0.82410562754\n",
      "epoch: 6 , training loss: 0.844939420627 , training acc: 0.812105562902 , valid loss: 0.797457032988 , valid acc: 0.82410562754\n",
      "epoch: 6 , pass acc: 0.82410562754 , current acc: 0.843349330709\n",
      "epoch: 7 , training loss: 0.761011689186 , training acc: 0.835515886116 , valid loss: 0.733151363225 , valid acc: 0.843349330709\n",
      "epoch: 7 , pass acc: 0.843349330709 , current acc: 0.85314525197\n",
      "epoch: 8 , training loss: 0.705745415374 , training acc: 0.848767240657 , valid loss: 0.687090511797 , valid acc: 0.85314525197\n",
      "epoch: 8 , pass acc: 0.85314525197 , current acc: 0.859339732225\n",
      "epoch: 9 , training loss: 0.66765936014 , training acc: 0.858122373099 , valid loss: 0.656822612485 , valid acc: 0.859339732225\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(maxlen, dimension, len(label), learning_rate, [2,3,4], size_layer)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 100, 0, 0, 0\n",
    "while True:\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:', EPOCH)\n",
    "        break\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    for i in range(0, (train_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = train_X[i + k].split()[:maxlen]\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    batch_x[k, -1 - no, :] += vectors[dictionary[text], :]\n",
    "                except:\n",
    "                    continue\n",
    "            batch_y[k, int(train_Y[i + k])] = 1.0\n",
    "        batch_x = np.expand_dims(batch_x, axis=-1)\n",
    "        loss, _ = sess.run([model.cost, model.optimizer], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        train_loss += loss\n",
    "        train_acc += sess.run(model.accuracy, feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "    \n",
    "    for i in range(0, (test_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = test_X[i + k].split()[:maxlen]\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    batch_x[k, -1 - no, :] += vectors[dictionary[text], :]\n",
    "                except:\n",
    "                    continue\n",
    "            batch_y[k, int(test_Y[i + k])] = 1.0\n",
    "        batch_x = np.expand_dims(batch_x, axis=-1)\n",
    "        loss, acc = sess.run([model.cost, model.accuracy], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "        \n",
    "    train_loss /= (train_X.shape[0] // batch)\n",
    "    train_acc /= (train_X.shape[0] // batch)\n",
    "    test_loss /= (test_X.shape[0] // batch)\n",
    "    test_acc /= (test_X.shape[0] // batch)\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch:', EPOCH, ', pass acc:', CURRENT_ACC, ', current acc:', test_acc)\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "        saver.save(sess, os.getcwd() + \"/model-cnn-vector.ckpt\")\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    EPOCH += 1\n",
    "    print('epoch:', EPOCH, ', training loss:', train_loss, ', training acc:', train_acc, ', valid loss:', test_loss, ', valid acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
