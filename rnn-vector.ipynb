{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Model_vec:\n",
    "    \n",
    "    def __init__(self, batch_size, dimension_size, learning_rate, vocabulary_size):\n",
    "        self.train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        self.train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, dimension_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, self.train_inputs)\n",
    "        self.nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, dimension_size], stddev = 1.0 / np.sqrt(dimension_size)))\n",
    "        self.nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        self.loss = tf.reduce_mean(tf.nn.nce_loss(weights = self.nce_weights, biases = self.nce_biases, labels = self.train_labels,\n",
    "                                                  inputs=embed, num_sampled = batch_size / 2, num_classes = vocabulary_size))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        self.norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        self.normalized_embeddings = embeddings / self.norm\n",
    "        \n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, num_layers, size_layer, dimension_input, dimension_output, learning_rate):\n",
    "        def lstm_cell():\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer)\n",
    "        self.rnn_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        self.X = tf.placeholder(tf.float32, [None, None, dimension_input])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(self.rnn_cells, output_keep_prob = 0.5)\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(drop, self.X, dtype = tf.float32)\n",
    "        self.rnn_W = tf.Variable(tf.random_normal((size_layer, dimension_output)))\n",
    "        self.rnn_B = tf.Variable(tf.random_normal([dimension_output]))\n",
    "        self.logits = tf.matmul(self.outputs[:, -1], self.rnn_W) + self.rnn_B\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
    "        l2 = sum(0.0005 * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "        self.cost += l2\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = re.sub('[^\\'\\\"A-Za-z0-9 ]+', '', string)\n",
    "    string = string.split(' ')\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    return ' '.join(string)\n",
    "\n",
    "def read_data():\n",
    "    list_folder = os.listdir('data/')\n",
    "    label = list_folder\n",
    "    label.sort()\n",
    "    outer_string, outer_label = [], []\n",
    "    for i in range(len(list_folder)):\n",
    "        list_file = os.listdir('data/' + list_folder[i])\n",
    "        strings = []\n",
    "        for x in range(len(list_file)):\n",
    "            with open('data/' + list_folder[i] + '/' + list_file[x], 'r') as fopen:\n",
    "                strings += fopen.read().split('\\n')\n",
    "        strings = list(filter(None, strings))\n",
    "        for k in range(len(strings)):\n",
    "            strings[k] = clearstring(strings[k])\n",
    "        labels = [i] * len(strings)\n",
    "        outer_string += strings\n",
    "        outer_label += labels\n",
    "    \n",
    "    dataset = np.array([outer_string, outer_label])\n",
    "    dataset = dataset.T\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    string = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        string += dataset[i][0].split()\n",
    "    \n",
    "    return string, dataset, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, vocabulary_size):\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) + 1\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        data.append(index)\n",
    "    dictionary['PAD'] = 0\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, dictionary, reverse_dictionary\n",
    "\n",
    "def generate_batch_skipgram(words, batch_size, num_skips, skip_window):\n",
    "    data_index = 0\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for i in range(span):\n",
    "        buffer.append(words[data_index])\n",
    "        data_index = (data_index + 1) % len(words)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(words[data_index])\n",
    "        data_index = (data_index + 1) % len(words)\n",
    "    data_index = (data_index + len(words) - span) % len(words)\n",
    "    return batch, labels\n",
    "\n",
    "def generatevector(dimension, batch_size, skip_size, skip_window, num_skips, iteration, words_real):\n",
    "    \n",
    "    print (\"Data size:\", len(words_real))\n",
    "    data, dictionary, reverse_dictionary = build_dataset(words_real, len(words_real))\n",
    "    sess = tf.InteractiveSession()\n",
    "    print (\"Creating Word2Vec model..\")\n",
    "    model = Model_vec(batch_size, dimension, 0.1, len(dictionary))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    last_time = time.time()\n",
    "    for step in range(iteration):\n",
    "        new_time = time.time()\n",
    "        batch_inputs, batch_labels = generate_batch_skipgram(data, batch_size, num_skips, skip_window)\n",
    "        feed_dict = {model.train_inputs: batch_inputs, model.train_labels: batch_labels}\n",
    "        _, loss = sess.run([model.optimizer, model.loss], feed_dict=feed_dict)\n",
    "        if ((step + 1) % 1000) == 0:\n",
    "            print (\"epoch:\", step + 1, \", loss:\", loss, \", speed:\", (time.time() - new_time) * 1000, \"s / 1000 epoch\")\n",
    "    tf.reset_default_graph()       \n",
    "    return dictionary, reverse_dictionary, model.normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string, data, label = read_data()\n",
    "maxlen = 50\n",
    "location = os.getcwd()\n",
    "dimension = 512\n",
    "skip_size = 8\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "iteration_train_vectors = 20000\n",
    "num_layers = 3\n",
    "size_layer = 256\n",
    "learning_rate = 0.0001\n",
    "epoch = 100\n",
    "batch = 100\n",
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 8007324\n",
      "Creating Word2Vec model..\n",
      "epoch: 1000 , loss: 63.1541 , speed: 75.82640647888184 s / 1000 epoch\n",
      "epoch: 2000 , loss: 46.7144 , speed: 76.01499557495117 s / 1000 epoch\n",
      "epoch: 3000 , loss: 17.3323 , speed: 75.90389251708984 s / 1000 epoch\n",
      "epoch: 4000 , loss: 41.815 , speed: 76.09033584594727 s / 1000 epoch\n",
      "epoch: 5000 , loss: 18.6723 , speed: 75.9592056274414 s / 1000 epoch\n",
      "epoch: 6000 , loss: 29.9499 , speed: 76.10583305358887 s / 1000 epoch\n",
      "epoch: 7000 , loss: 11.6105 , speed: 75.86288452148438 s / 1000 epoch\n",
      "epoch: 8000 , loss: 61.9361 , speed: 76.0037899017334 s / 1000 epoch\n",
      "epoch: 9000 , loss: 16.7285 , speed: 75.81090927124023 s / 1000 epoch\n",
      "epoch: 10000 , loss: 2.76245 , speed: 76.00092887878418 s / 1000 epoch\n",
      "epoch: 11000 , loss: 23.9825 , speed: 76.15137100219727 s / 1000 epoch\n",
      "epoch: 12000 , loss: 28.3714 , speed: 76.0793685913086 s / 1000 epoch\n",
      "epoch: 13000 , loss: 18.5455 , speed: 75.98590850830078 s / 1000 epoch\n",
      "epoch: 14000 , loss: 11.6234 , speed: 76.00998878479004 s / 1000 epoch\n",
      "epoch: 15000 , loss: 32.0073 , speed: 75.91032981872559 s / 1000 epoch\n",
      "epoch: 16000 , loss: 35.2264 , speed: 75.81686973571777 s / 1000 epoch\n",
      "epoch: 17000 , loss: 11.9644 , speed: 75.95705986022949 s / 1000 epoch\n",
      "epoch: 18000 , loss: 21.5655 , speed: 75.94871520996094 s / 1000 epoch\n",
      "epoch: 19000 , loss: 14.2862 , speed: 75.93154907226562 s / 1000 epoch\n",
      "epoch: 20000 , loss: 12.6238 , speed: 76.01690292358398 s / 1000 epoch\n"
     ]
    }
   ],
   "source": [
    "dictionary, reverse_dictionary, vectors = generatevector(dimension, 32, skip_size, skip_window, num_skips, iteration_train_vectors, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "model = Model(num_layers, size_layer, dimension, len(label), learning_rate)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data[:, 0], data[:, 1], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , pass acc: 0 , current acc: 0.54135652017\n",
      "epoch: 1 , training loss: 1.79305984192 , training acc: 0.499499083057 , valid loss: 1.60654118725 , valid acc: 0.54135652017\n",
      "epoch: 1 , pass acc: 0.54135652017 , current acc: 0.646254478955\n",
      "epoch: 2 , training loss: 1.47297949596 , training acc: 0.600779826779 , valid loss: 1.33500540214 , valid acc: 0.646254478955\n",
      "epoch: 2 , pass acc: 0.646254478955 , current acc: 0.738967566645\n",
      "epoch: 3 , training loss: 1.19271894438 , training acc: 0.709960987504 , valid loss: 1.09193496379 , valid acc: 0.738967566645\n",
      "epoch: 3 , pass acc: 0.738967566645 , current acc: 0.771296498298\n",
      "epoch: 4 , training loss: 1.00862628633 , training acc: 0.769283124302 , valid loss: 0.969972000188 , valid acc: 0.771296498298\n",
      "epoch: 4 , pass acc: 0.771296498298 , current acc: 0.795870330153\n",
      "epoch: 5 , training loss: 0.902573894028 , training acc: 0.79673663613 , valid loss: 0.874055074329 , valid acc: 0.795870330153\n",
      "epoch: 5 , pass acc: 0.795870330153 , current acc: 0.816566608104\n",
      "epoch: 6 , training loss: 0.819969141187 , training acc: 0.816673649898 , valid loss: 0.796124875689 , valid acc: 0.816566608104\n",
      "epoch: 6 , pass acc: 0.816566608104 , current acc: 0.833673455778\n",
      "epoch: 7 , training loss: 0.751848940121 , training acc: 0.835128963601 , valid loss: 0.732364212622 , valid acc: 0.833673455778\n",
      "epoch: 7 , pass acc: 0.833673455778 , current acc: 0.848715478919\n",
      "epoch: 8 , training loss: 0.696155158064 , training acc: 0.849553083991 , valid loss: 0.672429002169 , valid acc: 0.848715478919\n",
      "epoch: 8 , pass acc: 0.848715478919 , current acc: 0.855750296225\n",
      "epoch: 9 , training loss: 0.651919448866 , training acc: 0.860365925467 , valid loss: 0.639893154601 , valid acc: 0.855750296225\n",
      "epoch: 9 , pass acc: 0.855750296225 , current acc: 0.860228088223\n",
      "epoch: 10 , training loss: 0.616186546248 , training acc: 0.868563287635 , valid loss: 0.623462301843 , valid acc: 0.860228088223\n",
      "epoch: 10 , pass acc: 0.860228088223 , current acc: 0.868367349472\n",
      "epoch: 11 , training loss: 0.587229213397 , training acc: 0.875893824299 , valid loss: 0.587268264032 , valid acc: 0.868367349472\n",
      "epoch: 11 , pass acc: 0.868367349472 , current acc: 0.875870351746\n",
      "epoch: 12 , training loss: 0.562537003275 , training acc: 0.88135273541 , valid loss: 0.560900704056 , valid acc: 0.875870351746\n",
      "epoch: 12 , pass acc: 0.875870351746 , current acc: 0.877491000391\n",
      "epoch: 13 , training loss: 0.53956523459 , training acc: 0.88712358228 , valid loss: 0.546360418683 , valid acc: 0.877491000391\n",
      "epoch: 13 , pass acc: 0.877491000391 , current acc: 0.882713091044\n",
      "epoch: 14 , training loss: 0.537528355893 , training acc: 0.888797249187 , valid loss: 0.529290176311 , valid acc: 0.882713091044\n",
      "epoch: 14 , pass acc: 0.882713091044 , current acc: 0.887418979285\n",
      "epoch: 15 , training loss: 0.502063127955 , training acc: 0.89695561944 , valid loss: 0.512888554157 , valid acc: 0.887418979285\n",
      "epoch: 15 , pass acc: 0.887418979285 , current acc: 0.894501806021\n",
      "epoch: 16 , training loss: 0.481830559586 , training acc: 0.901769661178 , valid loss: 0.485961135982 , valid acc: 0.894501806021\n",
      "epoch: 16 , pass acc: 0.894501806021 , current acc: 0.898487401967\n",
      "epoch: 17 , training loss: 0.466697618365 , training acc: 0.905518907448 , valid loss: 0.469957934851 , valid acc: 0.898487401967\n",
      "epoch: 17 , pass acc: 0.898487401967 , current acc: 0.901956790922\n",
      "epoch: 18 , training loss: 0.455475770889 , training acc: 0.906682676552 , valid loss: 0.456880040529 , valid acc: 0.901956790922\n",
      "epoch: 18 , pass acc: 0.901956790922 , current acc: 0.902028830899\n",
      "epoch: 19 , training loss: 0.444016918558 , training acc: 0.909136187676 , valid loss: 0.449768354412 , valid acc: 0.902028830899\n",
      "epoch: 20 , training loss: 0.438225969243 , training acc: 0.909955015333 , valid loss: 0.45305646278 , valid acc: 0.901572648861\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a2e69b14e402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mbatch_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(num_layers, size_layer, dimension, len(label), learning_rate)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 100, 0, 0, 0\n",
    "batch_size = 200\n",
    "while True:\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:', EPOCH)\n",
    "        break\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    for i in range(0, (train_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = train_X[i + k].split()[:maxlen]\n",
    "            emb_data = np.zeros((maxlen, dimension), dtype = np.float32)\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    emb_data[-1 - no, :] += vectors[dictionary[text], :]\n",
    "                except:\n",
    "                    continue\n",
    "            batch_y[k, int(train_Y[i + k])] = 1.0\n",
    "            batch_x[k, :, :] = emb_data[:, :]\n",
    "        loss, _ = sess.run([model.cost, model.optimizer], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        train_loss += loss\n",
    "        train_acc += sess.run(model.accuracy, feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "    \n",
    "    for i in range(0, (test_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = test_X[i + k].split()[:maxlen]\n",
    "            emb_data = np.zeros((maxlen, dimension), dtype = np.float32)\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    emb_data[-1 - no, :] += vectors[dictionary[text], :]\n",
    "                except:\n",
    "                    continue\n",
    "            batch_y[k, int(test_Y[i + k])] = 1.0\n",
    "            batch_x[k, :, :] = emb_data[:, :]\n",
    "        loss, acc = sess.run([model.cost, model.accuracy], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "        \n",
    "    train_loss /= (train_X.shape[0] // batch)\n",
    "    train_acc /= (train_X.shape[0] // batch)\n",
    "    test_loss /= (test_X.shape[0] // batch)\n",
    "    test_acc /= (test_X.shape[0] // batch)\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch:', EPOCH, ', pass acc:', CURRENT_ACC, ', current acc:', test_acc)\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "        saver.save(sess, os.getcwd() + \"/model-rnn-vector.ckpt\")\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    EPOCH += 1\n",
    "    print('epoch:', EPOCH, ', training loss:', train_loss, ', training acc:', train_acc, ', valid loss:', test_loss, ', valid acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
